{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "style_transfer_2d.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobertArbon/art_of_md/blob/master/style_transfer_looped.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOP9mu4xepLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RBr8QbboRAdU",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from lucid.modelzoo import vision_models\n",
        "from lucid.misc.io import show, load, save\n",
        "from lucid.misc.tfutil import create_session\n",
        "import lucid.optvis.objectives as objectives\n",
        "import lucid.optvis.param as param\n",
        "import lucid.optvis.render as render\n",
        "from lucid.optvis.objectives import wrap_objective\n",
        "import lucid.modelzoo.nets_factory as nets\n",
        "from os.path import basename\n",
        "from itertools import product\n",
        "import pickle\n",
        "from django.utils.text import get_valid_filename\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDqlzRKmLPtY",
        "colab_type": "text"
      },
      "source": [
        "Globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D56ERpsaIn0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these constants help remember which image is at which batch dimension\n",
        "TRANSFER_INDEX = 0\n",
        "CONTENT_INDEX = 1\n",
        "STYLE_INDEX = 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdmxJ7KaZTh6",
        "colab_type": "code",
        "outputId": "b2b2f4b8-ab38-46dd-8790-7600e4d431ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!wget -nc http://francis-bacon.com/sites/default/files/2018-04/innocent-cropped.jpg"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘innocent-cropped.jpg’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzZtbOfiT8Dk",
        "colab_type": "text"
      },
      "source": [
        "Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiNDL_MYT68U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_layers = 5\n",
        "style_min = 0.01\n",
        "style_max = 0.2\n",
        "style_grid_n = 5\n",
        "model_name = 'VGG16_caffe'\n",
        "content_weight=100\n",
        "max_dim = 512\n",
        "\n",
        "\n",
        "content_path = \"3.jpg\"\n",
        "content_name = basename(content_path).split('.')[0]\n",
        "style_path = \"innocent-cropped.jpg\"\n",
        "style_name = basename(style_path).split('.')[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWB1e2KUM-AS",
        "colab_type": "text"
      },
      "source": [
        "Get model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbrtziqVLxRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nets.models_map[model_name]()\n",
        "model.load_graphdef()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yldn0vNcNAYl",
        "colab_type": "text"
      },
      "source": [
        "Param grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yNALaA0QRJVT",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "style_weights = np.logspace(np.log10(style_min), np.log10(style_max), style_grid_n)\n",
        "style_weights *= content_weight\n",
        "layers = [x.name for x in model.layers if 'conv' in x.tags]\n",
        "stride = int(np.floor(len(layers)/max_layers))\n",
        "layers = layers[::stride]\n",
        "params = list(product(style_weights, layers, layers))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TpFer0KYz-lE",
        "colab": {}
      },
      "source": [
        "assert tf.test.is_gpu_available(), 'No GPU!'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZFLAvbZqc0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_img(path_to_img, max_dim=512):\n",
        "  from PIL import Image\n",
        "  img = Image.open(path_to_img)\n",
        "  long = min(img.size)\n",
        "  scale = max_dim/long\n",
        "  img = np.asarray(img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), \n",
        "                   Image.ANTIALIAS))\n",
        "  img_max = np.iinfo(img.dtype).max\n",
        "  npimg = np.divide(img, img_max, dtype=np.float32)\n",
        "  return npimg\n",
        "\n",
        "def style_transfer_param(content_image, style_image, decorrelate=True, fft=True):\n",
        "  style_transfer_input = param.image(*content_image.shape[:2], decorrelate=decorrelate, fft=fft)[0]\n",
        "  content_input = content_image\n",
        "  style_input = tf.random_crop(style_image, content_image.shape)\n",
        "  return tf.stack([style_transfer_input, content_input, style_input])\n",
        "\n",
        "def mean_L1(a, b):\n",
        "  return tf.reduce_mean(tf.abs(a-b))\n",
        "\n",
        "@wrap_objective\n",
        "def activation_difference(layer_names, activation_loss_f=mean_L1, transform_f=None, difference_to=CONTENT_INDEX):\n",
        "  def inner(T):\n",
        "    # first we collect the (constant) activations of image we're computing the difference to\n",
        "    image_activations = [T(layer_name)[difference_to] for layer_name in layer_names]\n",
        "    if transform_f is not None:\n",
        "      image_activations = [transform_f(act) for act in image_activations]\n",
        "    \n",
        "    # we also set get the activations of the optimized image which will change during optimization\n",
        "    optimization_activations = [T(layer)[TRANSFER_INDEX] for layer in layer_names]\n",
        "    if transform_f is not None:\n",
        "      optimization_activations = [transform_f(act) for act in optimization_activations]\n",
        "    \n",
        "    # we use the supplied loss function to compute the actual losses\n",
        "    losses = [activation_loss_f(a, b) for a, b in zip(image_activations, optimization_activations)]\n",
        "    return tf.add_n(losses) \n",
        "    \n",
        "  return inner\n",
        "\n",
        "def gram_matrix(array, normalize_magnitue=True):\n",
        "  channels = tf.shape(array)[-1]\n",
        "  array_flat = tf.reshape(array, [-1, channels])\n",
        "  gram_matrix = tf.matmul(array_flat, array_flat, transpose_a=True)\n",
        "  if normalize_magnitue:\n",
        "    length = tf.shape(array_flat)[0]\n",
        "    gram_matrix /= tf.cast(length, tf.float32)\n",
        "  return gram_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VEosbhdqc0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_image = load_img(content_path, max_dim=max_dim)\n",
        "style_image = load_img(style_path, max_dim=max_dim) \n",
        "param_f = lambda: style_transfer_param(content_image, style_image)\n",
        "for i in range(len(params[:3])):\n",
        "  # get params\n",
        "  style_weight = params[i][0]\n",
        "  content_layers = [params[i][1]]\n",
        "  style_layers = [params[i][2]]\n",
        "\n",
        "  # set up objective functions\n",
        "  content_obj = content_weight * activation_difference(content_layers, difference_to=CONTENT_INDEX)\n",
        "  content_obj.description = \"Content Loss\"\n",
        "  style_obj = style_weight*activation_difference(style_layers, transform_f=gram_matrix, difference_to=STYLE_INDEX)\n",
        "  style_obj.description = \"Style Loss\"\n",
        "  objective = - content_obj - style_obj\n",
        "\n",
        "  # do learning\n",
        "  vis = render.render_vis(model, objective, param_f=param_f, thresholds=[128, 256], verbose=False)\n",
        "  \n",
        "  # package results\n",
        "  results = {'style_layers': style_layers, 'content_layers': content_layers, \n",
        "           'style_weight': style_weight, 'content_weight': content_weight, \n",
        "           'model': model_name, 'image': np.array(vis), \n",
        "            'style_name': style_name, 'content_name': content_name}\n",
        "\n",
        "  # save results\n",
        "  fname = '{0}_{1}_{2}_{3}_W{4:4.2f}_{5}_W{6:4.2f}.p'.format(results['model'], \n",
        "                                                           results['content_name'], \n",
        "                                                          results['style_name'], \n",
        "                                                          results['content_layers'], \n",
        "                                                          results['content_weight'], \n",
        "                                                          results['style_layers'], \n",
        "                                                          results['style_weight'])\n",
        "  fname = get_valid_filename(fname)\n",
        "  pickle.dump(obj=results, file=open(fname, 'wb'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}